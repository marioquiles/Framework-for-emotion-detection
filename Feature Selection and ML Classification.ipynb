{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import seaborn as sn \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# In our case, each of the datasets corresponding to the emotions to be predicted is located in different files.\n",
    "\n",
    "allDataJoy = pd.read_csv(\"ExperimentoMarioPostFeaturesSim/joy_procesedSimulador.csv\")  \n",
    "allDataAngry = pd.read_csv('ExperimentoMarioPostFeaturesSim/angry_procesedSimulador.csv')  \n",
    "allDataNeutral = pd.read_csv('ExperimentoMarioPostFeaturesSim/neutral_procesedSimulador.csv')  \n",
    "allDataNoStim = pd.read_csv('ExperimentoMarioPostFeaturesSim/nada_procesedSimulador.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assign the labels to each of the datasets\n",
    "\n",
    "allDataNoStim = allDataNoStim.assign(Emotion = 0)\n",
    "allDataAngry = allDataAngry.assign(Emotion = 1)\n",
    "allDataNeutral = allDataNeutral.assign(Emotion = 2)\n",
    "allDataJoy = allDataJoy.assign(Emotion = 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next we are going to eliminate the correlated variables and apply the PCA algorithm for feature reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allData = pd.concat([allDataNoStim, allDataAngry, allDataNeutral, allDataJoy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allData = allData.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "cor_matrix  = allData.corr().abs()\n",
    "\n",
    "# Select columns that are more than 95% correlated\n",
    "upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.90)]\n",
    "\n",
    "# Remove the columns selected from the dataframe\n",
    "allDataPostCor = allData.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = allDataPostCor[\"Emotion\"]\n",
    "allDataPostCor = allDataPostCor.drop([\"Emotion\"], axis=1)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(allDataPostCor)\n",
    "\n",
    "# Apply PCA to the scaled data\n",
    "pca = PCA(0.95)\n",
    "pca_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Create a new dataframe with the PCA data\n",
    "allDataPCA = pd.DataFrame(data=pca_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the label column\n",
    "\n",
    "allDataPCA[\"Emotion\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We separate the dataframe again to be able to perform the train test data divisions correctly\n",
    "\n",
    "allDataJoy = allDataPCA.loc[allDataPCA['Emotion'] == 3]\n",
    "allDataNoStim = allDataPCA.loc[allDataPCA['Emotion'] == 0]\n",
    "allDataNeutral = allDataPCA.loc[allDataPCA['Emotion'] == 2]\n",
    "allDataAngry = allDataPCA.loc[allDataPCA['Emotion'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We divide each of the datasets into train and test to have balanced training and validation sets\n",
    "\n",
    "joyCopia = allDataJoy.copy()\n",
    "angerCopia = allDataAngry.copy()\n",
    "neutralCopia = allDataNeutral.copy()\n",
    "nadaCopia = allDataNoStim.copy()\n",
    "\n",
    "\n",
    "yJoy = joyCopia[['Emotion']]\n",
    "\n",
    "xJoy = joyCopia.drop(columns = [\"Emotion\"])\n",
    "\n",
    "X_trainJoy, X_testJoy, y_trainJoy, y_testJoy = train_test_split(xJoy,yJoy, test_size=0.20, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yAngry = angerCopia[['Emotion']]\n",
    "\n",
    "xAngry = angerCopia.drop(columns = [\"Emotion\"])\n",
    "\n",
    "X_trainAngry, X_testAngry, y_trainAngry, y_testAngry = train_test_split(xAngry,yAngry, test_size=0.20, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yNeutral = neutralCopia[['Emotion']]\n",
    "\n",
    "xNeutral = neutralCopia.drop(columns = [\"Emotion\"])\n",
    "\n",
    "X_trainNeutral, X_testNeutral, y_trainNeutral, y_testNeutral = train_test_split(xNeutral,yNeutral, test_size=0.20, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yNada = nadaCopia[['Emotion']]\n",
    "\n",
    "xNada = nadaCopia.drop(columns = [\"Emotion\"])\n",
    "\n",
    "X_trainNada, X_testNada, y_trainNada, y_testNada = train_test_split(xNada,yNada, test_size=0.20, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.concat([X_trainJoy.copy(), X_trainAngry.copy(), X_trainNeutral.copy(), X_trainNada.copy()], axis=0)\n",
    "y_train = pd.concat([y_trainJoy.copy(), y_trainAngry.copy(), y_trainNeutral.copy(), y_trainNada.copy()], axis=0)\n",
    "x_test = pd.concat([X_testJoy,X_testAngry,X_testNeutral, X_testNada], axis=0)\n",
    "y_test = pd.concat([y_testJoy,y_testAngry,y_testNeutral, y_testNada], axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Training with random forest algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier()\n",
    "rnd_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion = rnd_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, prediccion))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter test is very expensive with the type of data we have, since each training takes around 7 minutes with random forest. So a test with 9 different parameters using gridsearch will take months, even parallelizing it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is followed by training with the XGBoost algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_mat = xgb.DMatrix(x_train, label=y_train)\n",
    "\n",
    "test_mat = xgb.DMatrix(x_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"booster\":\"gbtree\", \"max_depth\": 2, \"eta\": 0.3, \"objective\": \"multi:softmax\", \"nthread\":2, 'num_class':4}\n",
    "rounds = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = [(test_mat, \"eval\"), (train_mat, \"train\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = xgb.train(params, train_mat, rounds, eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion = modelo.predict(test_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, prediccion))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the training of the kNN model is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Crear un diccionario con los valores a probar para el hiperparámetro k\n",
    "param_grid = {'n_neighbors': [5, 7, 9, 11, 13]}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Crear una instancia de la clase GridSearchCV\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "# Entrenar el modelo usando la búsqueda de hiperparámetros\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Imprimir el mejor valor de k encontrado\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "knn.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion = knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, prediccion))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the trainings for the ternary model with the classes Angry, Neutral and No-Stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data with three classes\n",
    "\n",
    "x_train = pd.concat([ X_trainAngry.copy(), X_trainNeutral.copy(), X_trainNada.copy()], axis=0)\n",
    "y_train = pd.concat([ y_trainAngry.copy(), y_trainNeutral.copy(), y_trainNada.copy()], axis=0)\n",
    "x_test = pd.concat([X_testAngry,X_testNeutral, X_testNada], axis=0)\n",
    "y_test = pd.concat([y_testAngry,y_testNeutral, y_testNada], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "rnd_clf = RandomForestClassifier()\n",
    "rnd_clf.fit(x_train, y_train)\n",
    "\n",
    "prediccion = rnd_clf.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, prediccion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost model\n",
    "\n",
    "train_mat = xgb.DMatrix(x_train, label=y_train)\n",
    "test_mat = xgb.DMatrix(x_test, label=y_test)\n",
    "\n",
    "params = {\"booster\":\"gbtree\", \"max_depth\": 2, \"eta\": 0.3, \"objective\": \"multi:softmax\", \"nthread\":2, 'num_class':3}\n",
    "rounds = 10\n",
    "\n",
    "eval = [(test_mat, \"eval\"), (train_mat, \"train\")]\n",
    "\n",
    "model = xgb.train(params, train_mat, rounds, eval)\n",
    "\n",
    "prediction = model.predict(test_mat)\n",
    "\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN model\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "prediction = knn.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.concat([ X_trainAngry.copy(),  X_trainNada.copy()], axis=0)\n",
    "y_train = pd.concat([ y_trainAngry.copy(), y_trainNada.copy()], axis=0)\n",
    "x_test = pd.concat([X_testAngry, X_testNada], axis=0)\n",
    "y_test = pd.concat([y_testAngry, y_testNada], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter space to search over\n",
    "param_dist = {\"n_estimators\": [10, 100, 1000],\n",
    "              \"max_depth\": [3, 5, 10],\n",
    "              \"min_samples_split\": [2, 3, 10]}\n",
    "\n",
    "# Create a random forest classifier\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Use RandomizedSearchCV to search over the hyperparameter space\n",
    "search = RandomizedSearchCV(rfc, param_dist, n_jobs=4)\n",
    "\n",
    "# Fit the model on the training data\n",
    "search.fit(x_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best hyperparameters:\", search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=20, max_depth=10)\n",
    "rnd_clf.fit(x_train, y_train)\n",
    "\n",
    "prediccion = rnd_clf.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, prediccion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost model\n",
    "\n",
    "# Init classifier\n",
    "xgb_cl = xgb.XGBClassifier(objective=\"binary:logistic\")\n",
    "\n",
    "# Fit\n",
    "xgb_cl.fit(x_train, y_train)\n",
    "\n",
    "# Predict\n",
    "prediction = xgb_cl.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN model\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "prediction = knn.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "706a7eb66341221f9f9a9abd7958ed5e6a512fe3731f272ffc19509b3625a4f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
